{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcb76341-f43d-41f9-9628-5ec790577cab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Try to use this sales data with atleast very important and important options...\n",
    "https://drive.google.com/file/d/1MZI4XIofL-0QpMIr9sFODSKVkexrSZ1A/view?usp=drive_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "265dbc51-b1e7-47d8-b864-6877bdc08546",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###1. CSV Advanced Features - \n",
    "######Very Important - path: PathOrPaths, schema: Optional[Union[StructType, str]]=None, sep: Optional[str]=None,header: Optional[Union[bool, str]]=None, inferSchema: Optional[Union[bool, str]]=None, \n",
    "######Important - mode: Optional[str]=None, columnNameOfCorruptRecord: Optional[str]=None,  quote: Optional[str]=None, escape: Optional[str]=None, \n",
    "Not Important but good to know once - encoding: Optional[str]=None, comment: Optional[str]=None,ignoreLeadingWhiteSpace: Optional[Union[bool, str]]=None, ignoreTrailingWhiteSpace: Optional[Union[bool, str]]=None, nullValue: Optional[str]=None, nanValue: Optional[str]=None, positiveInf: Optional[str]=None, negativeInf: Optional[str]=None, dateFormat: Optional[str]=None, timestampFormat: Optional[str]=None, maxColumns: Optional[Union[int, str]]=None, maxCharsPerColumn: Optional[Union[int, str]]=None, maxMalformedLogPerPartition: Optional[Union[int, str]]=None,   multiLine: Optional[Union[bool, str]]=None, charToEscapeQuoteEscaping: Optional[str]=None, samplingRatio: Optional[Union[float, str]]=None, enforceSchema: Optional[Union[bool, str]]=None, emptyValue: Optional[str]=None, locale: Optional[str]=None, lineSep: Optional[str]=None, pathGlobFilter: Optional[Union[bool, str]]=None, recursiveFileLookup: Optional[Union[bool, str]]=None, modifiedBefore: Optional[Union[bool, str]]=None, modifiedAfter: Optional[Union[bool, str]]=None, unescapedQuoteHandling: Optional[str]=None) -> \"DataFrame\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c791525-16e3-4844-878c-231d8df6d816",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### A. Options for handling quotes & Escape\n",
    "\n",
    "id,name,remarks<br>\n",
    "1,'Ramesh, K.P','Good performer'<br>\n",
    "2,'Manoj','Needs ~'special~' attention'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71c724b5-9e63-43b8-915f-0253e6cea121",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#When to go for quote: If the data is having delimiter in it..\n",
    "#When to go for escape: If the data is having quote in it...\n",
    "struct1=\"custid int,name string,age int,corrupt_record string\"\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/workspace/default/volumewd36/malformeddata1.txt\",header=False,sep=',',mode='permissive',comment='#',columnNameOfCorruptRecord=\"corrupt_record\",quote=\"'\",escape=\"|\")\n",
    "df1.show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a32848fc-80c5-484b-8631-a4b51315eacf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### B. Comments, Multi line, leading and trailing whitespace handling, null and nan handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab548c3c-891b-456d-96ce-9846dd82b6e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "struct1=\"custid int,name string,height float,joindt date,age string\"\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/workspace/default/volumewd36/malformeddata2.txt\",header=False,mode='permissive'\n",
    "                                   ,multiLine=True,quote=\"'\",ignoreLeadingWhiteSpace=True,ignoreTrailingWhiteSpace=True,\n",
    "                                   nullValue='na',nanValue=-1,maxCharsPerColumn='100',modifiedAfter='2025-12-19',dateFormat=\"yyyy-dd-MM\")\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5d88a14-9891-42ed-bea0-20adc7ebe32a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### C. Read modes in csv (Important feature)\n",
    "If any data challenges (malformed data) such as format issue/column numbers (lesser/more than expected) issue etc.,\n",
    "### There are 3 typical read modes and the default read mode is permissive.\n",
    "##### 1. permissive — All fields are set to null and corrupted records are placed in a string column called _corrupt_record\n",
    "##### \t2. dropMalformed — Drops all rows containing corrupt records.\n",
    "##### 3. failFast — Fails when corrupt records are encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91f180e5-9cb3-43be-80ec-093af5b4f00e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We learned about few important features mode, columnNameOfCorruptRecord, Quote, Comment\n",
    "#Question - Corrupt_record column consume more memory because it capturing all the column values(incorrect) in one column. ? Useful for doing RCA (Root Cause Analysis/Debugging)\n",
    "struct1=\"custid int,name string,age int,corrupt_record string\"\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/workspace/default/volumewd36/malformeddata.txt\",header=False,sep=',',mode='permissive',comment='#',columnNameOfCorruptRecord=\"corrupt_record\",quote=\"'\")\n",
    "df1.show(10)\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/workspace/default/volumewd36/malformeddata.txt\",header=False,sep=',',mode='dropMalformed',comment='#',columnNameOfCorruptRecord=\"corrupt_record\",quote=\"'\")\n",
    "df1.show(10)\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/workspace/default/volumewd36/malformeddata.txt\",header=False,sep=',',mode='failFast',comment='#',columnNameOfCorruptRecord=\"corrupt_record\",quote=\"'\")\n",
    "df1.show(10)\n",
    "\n",
    "#df1.filter(\"corrupt_record is not null\").write.csv(\"/Volumes/workspace/default/volumewd36/rejecteddata\")\n",
    "#spark.read.csv(\"/Volumes/workspace/default/volumewd36/rejecteddata\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccc9e257-4352-480d-8cce-47327b6f9a9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###2. JSON Advanced Features - \n",
    "**Very Important** - path,schema,columnNameOfCorruptRecord,dateFormat,timestampFormat,multiLine,pathGlobFilter,recursiveFileLookup<br>\n",
    "No header, No inferSchema, No sep in json...<br>\n",
    "**Important** - primitivesAsString(don't do inferSchema), prefersDecimal, allowComments, allowUnquotedFieldNames, `allowSingleQuotes`, lineSep, samplingRatio, dropFieldIfAllNull, modifiedBefore, modifiedAfter, useUnsafeRow(This is performance optimization when the data is loaded into spark memory) <br>\n",
    "**Not Important** (just try to know once for all) - allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, allowUnquotedControlChars, encoding, locale, allowNonNumericNumbers<br>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Advanced_Readops",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
